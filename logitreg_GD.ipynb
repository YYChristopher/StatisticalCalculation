{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    class Model:\n",
    "        \"\"\"\n",
    "        desc:训练出的模型\n",
    "        \"\"\"\n",
    "        def __init__(self,theta):\n",
    "            \"\"\"\n",
    "            desc:根据传入的 θ 构建模型 \n",
    "            \"\"\"\n",
    "            self.theta = theta\n",
    "        def predict(self,x_test):\n",
    "            \"\"\"\n",
    "            desc:根据传入的特征,利用模型预测数据\n",
    "            \"\"\"\n",
    "            t = np.dot(x_test, self.theta)\n",
    "            # 得出类别为 1 的概率\n",
    "            result = LogisticRegression.sigmoid(t)\n",
    "            # 将所有数据取整  p > 0.5 return: 1 , p <= 0.5 return: 0\n",
    "            return np.round(result)\n",
    "    \n",
    "    def __init__(self,alpha,theta,valve,batch_n=-1,max_iter=10**3):\n",
    "        \"\"\"\n",
    "        :param: alpha: 学习率,也称梯度下降中每一步的步长\n",
    "        :param: theta: 初始化的 θ 向量\n",
    "        :param: valve: 阀值,训练过程中,如梯度值的绝对值小于阀值就会跳出训练\n",
    "        :param: batch_n: 取一个整数表示微梯度下降中采用的样本数量,-1表示每次迭代采用全部数据\n",
    "        :param: max_iter: 最大的迭代次数,超过时自动跳出训练\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.theta = theta\n",
    "        self.valve = valve\n",
    "        self.batch_n = batch_n\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit(self,x_data,y_data):\n",
    "        \"\"\"\n",
    "        :param x_data: 特征值\n",
    "        :param y_data: 标签值\n",
    "        \"\"\"\n",
    "        iter_cnt = 0  #初始化迭代次数为0\n",
    "        data_len = x_data.shape[0]  #样本量n\n",
    "        # 验证样参数是否满足要求,并使其满足要求\n",
    "        self.batch_n = data_len if self.batch_n <= 0 or self.batch_n > data_len else self.batch_n\n",
    "        # 开始迭代\n",
    "        while self.__gradient_func(x_data,y_data) and iter_cnt < self.max_iter:\n",
    "            iter_cnt += 1\n",
    "        return self.Model(self.theta)\n",
    "    \n",
    "    def __gradient_func(self,x_data,y_data):\n",
    "        \"\"\"\n",
    "        :param x_data: 特征值\n",
    "        :param y_data: 标签值\n",
    "        \"\"\"\n",
    "        gradient_vector = np.zeros(self.theta.size)  #梯度向量每一步初始化为全0向量\n",
    "        data_len = x_data.shape[0]  #样本量n\n",
    "        fetature_len = self.theta.shape[0]  #特征数p\n",
    "        \n",
    "        # 遍历特征维度\n",
    "        for t in range(fetature_len):\n",
    "            j_ = 0\n",
    "            # 遍历样本数据\n",
    "            for i in range(0,data_len,round(data_len/self.batch_n)):  #按照批量进行计算\n",
    "                # 计算θx_1 + θx_2 + ... + θx_n    也就是公式中的(θ^TX)\n",
    "                t1 = np.dot(x_data[i], self.theta)\n",
    "                # 计算预测值 h_θ(x_i)\n",
    "                h_theta = LogisticRegression.sigmoid(t1)\n",
    "                # 结果累加起来 (h_θ(x_i) - y_i)x_ij\n",
    "                j_ += (h_theta-y_data[i])*x_data[i][t]\n",
    "            # 除以处理的样本数量,也就是取均值\n",
    "            j_ /= self.batch_n\n",
    "            # 修改梯度下降方向向量中对应的值\n",
    "            gradient_vector[t] = j_\n",
    "        \n",
    "        # 更新全局 θ 的值,也就是朝着下山的方向走一步\n",
    "        self.theta = self.theta - self.alpha*gradient_vector\n",
    "        # 输出梯度是否达到阀值\n",
    "        return np.abs(gradient_vector.sum()) >= self.valve\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        t = 1 + np.exp(-x)\n",
    "        result = np.divide(1,t)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = np.column_stack(load_iris(return_X_y=True))\n",
    "# 这里我们实现的只是二分类算法,左右只需要类别0和类别1两种即可\n",
    "iris_data = iris_data[iris_data[:,-1]<2]\n",
    "# 将数据打乱\n",
    "np.random.shuffle(iris_data)\n",
    "# 将数据分成两份\n",
    "train_data,test_data= np.split(iris_data,2,axis=0)\n",
    "# 训练数据\n",
    "train_x = train_data[:,:-1]\n",
    "train_y = train_data[:,-1]\n",
    "# 测试数据\n",
    "test_x = test_data[:,:-1]\n",
    "test_y = test_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LogisticRegression(0.04,np.array([1]*train_x.shape[1]),0.0003,batch_n=20,max_iter=1000)\n",
    "m = l.fit(train_x,train_y)\n",
    "pred_y = m.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率为:1.0000\n",
      "预测查准率为:1.0000\n",
      "预测召回率为:1.0000\n",
      "预测f1-score为:1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "acc = accuracy_score(test_y,pred_y)\n",
    "rec = recall_score(test_y,pred_y)\n",
    "pre = precision_score(test_y,pred_y)\n",
    "f1 = f1_score(test_y,pred_y)\n",
    "\n",
    "print('预测准确率为:{:.4f}'.format(acc))\n",
    "print('预测查准率为:{:.4f}'.format(pre))\n",
    "print('预测召回率为:{:.4f}'.format(rec))\n",
    "print('预测f1-score为:{:.4f}'.format(f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16f5b46f222e2a3e8d4adbf7141cae37b71ed37616e60735fa5d1164a1bc3ada"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
